llm_providers:
  - name: my_bam
    type: bam
    url: "https://bam-api.res.ibm.com"
    credentials_path: bam_api_key.txt
    models:
      - name: ibm/granite-13b-chat-v2
        context_window_size: 8000
        parameters:
          max_tokens_for_response: 500
  - name: my_openai
    type: openai
    url: "https://api.openai.com/v1"
    credentials_path: openai_api_key.txt
    models:
      - name: gpt-4-1106-preview
      - name: gpt-3.5-turbo
  - name: my_azure_openai
    type: azure_openai
    url: "https://myendpoint.openai.azure.com/"
    credentials_path: azure_openai_api_key.txt
    deployment_name: my_azure_openai_deployment_name
    models:
      - name: gpt-3.5-turbo
  - name: my_watsonx
    type: watsonx
    url: "https://us-south.ml.cloud.ibm.com"
    credentials_path: watsonx_api_key.txt
    project_id: XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX
    models:
      - name: ibm/granite-13b-chat-v2
  - name: my_rhoai
    type: rhoai_vllm
    url: "http://localhost:8000/v1"
    credentials_path: rhoai_api_key.txt
    models:
      - name: mistral-7b-instruct-v0.3
  - name: my_rhelai
    type: rhelai_vllm
    url: "http://localhost:8000/v1"
    credentials_path: rhelai_api_key.txt
    models:
      - name: granite-13b-chat-v2
  - name: instructlab
    type: openai
    url: "http://localhost:8000/v1"
    credentials_path: openai_api_key.txt
    models:
      - name: merlinite-7b-lab-Q4_K_M
ols_config:
  reference_content:
#    product_docs_index_path: "./vector_db/ocp_product_docs/4.15"
#    product_docs_index_id: ocp-product-docs-4_15
#    embeddings_model_path: "./embeddings_model"
  conversation_cache:
    type: memory
    memory:
      max_entries: 1000
  logging_config:
    app_log_level: info
    lib_log_level: warning
    uvicorn_log_level: info
  default_provider: my_bam
  default_model: ibm/granite-13b-chat-v2
  # query_filters:
  #   - name: foo_filter
  #     pattern: '\b(?:foo)\b'
  #     replace_with: "deployment"
  #   - name: bar_filter
  #     pattern: '\b(?:bar)\b'
  #     replace_with: "openshift"
  query_validation_method: llm
  authentication_config:
    k8s_cluster_api: "https://api.example.com:6443"
    k8s_ca_cert_path: "/Users/home/ca.crt"
    skip_tls_verification: false
  user_data_collection:
    feedback_disabled: false
    feedback_storage: "/tmp/data/feedback"
    transcripts_disabled: false
    transcripts_storage: "/tmp/data/transcripts"
  tls_config:
    tls_certificate_path: /app-root/certs/certificate.crt
    tls_key_path: /app-root/certs/private.key
    tls_key_password_path: /app-root/certs/password.txt
dev_config:
  # config options specific to dev environment - launching OLS in local
  enable_dev_ui: true
  disable_auth: true
  disable_tls: true
  pyroscope_url: "https://pyroscope.pyroscope.svc.cluster.local:4040"
  # llm_params:
  #   temperature_override: 0
  # k8s_auth_token: optional_token_when_no_available_kube_config
