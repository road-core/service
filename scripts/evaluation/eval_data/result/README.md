# Evaluation Result

**Notes**
- QnAs should `not` be used for model training or tuning. This is created only for evaluation purpose.
- QnAs were generated from OCP docs by LLMs. It is possible that some of the questions/answers are not entirely correct. We are constantly trying to verify both Questions & Answers manually. If you find any QnA pair to be modified or removed, please create a PR.
- [Instruction to run eval script](../../README.md)

## Result
- Last Execution date: `2024-11-30`
- Evaluated Provider/Models:
    - (watsonx) ibm/granite-3-8b-instruct
    - (openai) gpt-4o-mini (Model Version: 2024-07-18)
    - (azure) gpt-4o-mini (Model Version: 2024-07-18, API Version: 2024-02-15-preview)
- Judge provider/model (LLM based eval): (openai) gpt-4o-mini
- QnA evaluation dataset: [QnAs from OCP doc](../ocp_doc_qna-edited.parquet)

### Scores
Please look at below box-plots to have an idea about overall score for QnAs. `Ideal result`: Box should be as condense as possible and near to right side (close to 1).
#### Correctness/Similarity score generated by Model
Here we ask a judge model to score/grade response generated by OLS against the pre-defined/ground-truth answer.
![Similarity Score by LLM](model_evaluation_result-answer_similarity_llm.png)
#### Cosine Similarity
Cosine similarity between vector representation (generated by embedding model) of ground-truth answer & OLS response.
![Cosine Similarity](model_evaluation_result-cos_score.png)
#### Answer Relevancy
Here we generate multiple questions from the response using a Model and calculate average cosine similarity score between the original query & the generated questions.
![Answer Relevancy score](model_evaluation_result-answer_relevancy.png)
#### Rouge-L
These scores are based on stem words. Here the longest common subsequence (LCS) is being considered (not necessarily in order) between the predefined answer and the OLS response.
##### Rouge-L - Precision
Ratio of the length of the LCS over the unigrams in the response.
![Rouge-L Precison score](model_evaluation_result-rougeL_precision.png)
##### Rouge-L - Recall
Ratio of the length of the LCS over the unigrams in the pre-defined answer.
![Rouge-L Recall score](model_evaluation_result-rougeL_recall.png)
##### Rouge-L - F1
Harmonic mean of precision & recall.
![Rouge-L F1 score](model_evaluation_result-rougeL_f1.png)

[Score Summary](model_evaluation_summary.json)
Json file contains different quantile values for each of the score.